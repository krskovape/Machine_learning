{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ___\n",
    "\n",
    "# [ Machine Learning in Geosciences ]\n",
    "\n",
    "**Department of Applied Geoinformatics and Carthography, Charles University** \n",
    "\n",
    "*Lukas Brodsky lukas.brodsky@natur.cuni.cz*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises: Regularized Polynomial Regression \n",
    "\n",
    "\n",
    "Input data: X .. change of water level of a dam, y .. water flowing out of the dam. \n",
    "\n",
    "Goal: model the regression relationshop. Generalize the polynomial model by explicit regularization of the Loss. \n",
    "\n",
    "Tasks: \n",
    "\n",
    "1. Develop Loss function with $l2$ regularization term for polynomial regression;  \n",
    "\n",
    "2. Run linear regression, plot the resulting model and learning curve (training and testing error agains the number of samples); \n",
    "\n",
    "3. Run polynomial regression (degree = 5), plot the resulting model, and the learning curve; \n",
    "\n",
    "4. Regularized the model with lambda = 2, plot the resulting model, and the learning curve; \n",
    "\n",
    "5. Regularized the model with lambda = 100, plot the resulting model, and the learning curve; \n",
    "\n",
    "6. Discuss the effect of regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uzivatel\\AppData\\Local\\Temp\\ipykernel_5648\\999694006.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# optimization procedure\n",
    "from scipy.optimize import minimize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (12, 2)\n",
      "y_train: (12, 1)\n",
      "X_test: (21, 2)\n",
      "y_test: (21, 1)\n"
     ]
    }
   ],
   "source": [
    "data = loadmat('./ex7data.mat')\n",
    "X_train = np.c_[np.ones_like(data['X']), data['X']]\n",
    "y_train = data['y']\n",
    "\n",
    "X_test = np.c_[np.ones_like(data['Xval']), data['Xval']]\n",
    "y_test = data['yval']\n",
    "\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the input data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Water flowing out of the dam (y)')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot input data \n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X_train[:, 1], y_train, s = 50, c = 'red', marker = 'o', linewidths = 1, label = 'Data')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Change in water level (x)')\n",
    "plt.ylabel('Water flowing out of the dam (y)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: \n",
    "The data are definitely not linear, but let's define the linear regression loss function with a second term, the regularization term that controls overfitting and underfitting. A higher $\\lambda$ value means smaller parameter ($\\theta$) values (danger of underfitting) and vice versa (danger of overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Linear Regression Loss Function\n",
    "\n",
    "$$L(\\theta) = \\frac{1}{2m} \\Big(\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2\\Big) + \\frac{\\lambda}{2m}\\Big(\\sum_{j=1}^n \\theta_j^2\\Big)$$\n",
    "\n",
    "where $h_\\theta(x)$ is the linear model defined by:\n",
    "\n",
    "$$ h_\\theta(x) = \\theta^Tx = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... +  \\theta_n x_n $$\n",
    "\n",
    "where $m$ is the number of training examples, $n$ is the number of parameters, and $\\theta_0$ is the bias term which goes unregularized.\n",
    "\n",
    "In order to minimize this loss function, we also need to calculate the partial derivatives of $L$ with respect to each $\\theta_{j}$.\n",
    "\n",
    "### Linear Regression Partial Derivatives\n",
    "$ \\frac{\\partial L(\\theta)}{\\partial\\theta_{j}} = \\frac{1}{m}\\sum_{i=1}^{m} \\Big( h_\\theta (x^{(i)})-y^{(i)}\\Big)x^{(i)}_{j} $      for $ j = 0 $\n",
    "\n",
    "$ \\frac{\\partial L(\\theta)}{\\partial\\theta_{j}} = \\Big(\\frac{1}{m}\\sum_{i=1}^{m}  ( h_\\theta (x^{(i)})-y^{(i)})x^{(i)}_{j}\\Big) + \\frac{\\lambda}{m}\\theta_j$ for $ j \\geq1 $\n",
    "\n",
    "These definitions are different because we do not regularize $\\theta_0$, the bias term.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1. Develop Loss function with $l2$ regularization term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function taking into account the regularization term initialized to 0 \n",
    "# L_total = Lmse + L_reg \n",
    "\n",
    "def loss(X, y, theta, reg=0):\n",
    "    m = y.size\n",
    "    # the model\n",
    "    h = np.dot(X,theta)\n",
    "    # losses\n",
    "    L_mse = (1 / 2 * m) * np.sum(np.square(y-h))\n",
    "    L_reg = 0\n",
    "    L_total = L_mse + L_reg\n",
    "\n",
    "    return L_total "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2loss(w, x,y, reg):\n",
    "    total_error =  0\n",
    "    for i in range(len(x)):\n",
    "        mse = ((y[i] - (w*x[i]))**2)/float(len(x))\n",
    "        l2 = reg * (w**2)\n",
    "        total_error += mse + l2\n",
    "    return total_error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's  initialize theta as a vector of ones with a dimensionality equal to the number of columns in our matrix $X$. Then we can just pass the function these variables and see what it outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: (12, 1)\n",
      "y_train: (12, 1)\n",
      "X_train: (12, 2)\n",
      "theta: (2, 1)\n",
      "loss: 20297.393406799765\n"
     ]
    }
   ],
   "source": [
    "# Test the losss() function with some data \n",
    "\n",
    "# Initialize theta\n",
    "#theta = np.ones(X_train.shape[1])\n",
    "theta = np.zeros(X_train.shape[1]).reshape(-1,1)\n",
    "\n",
    "h =np.dot(X_train, theta)\n",
    "\n",
    "print('h:', h.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_train:', X_train.shape)\n",
    "print('theta:', theta.shape)\n",
    "\n",
    "print('loss:', loss(X_train, y_train, theta, 0)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimize\n",
    "\n",
    "Now, we optimize our parameter values using the scipy function `minimize()`, taking as parameters: \n",
    "\n",
    "`fun = loss` .. loss function, as defined \n",
    "\n",
    "`x0 = theta` .. initial values of model parmeters theta \n",
    "\n",
    "`args = (X, y, reg)` inputs \n",
    "\n",
    "`jac = True` .. jac : bool or callable, optional Jacobian (gradient) of objective function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimization procedure using scipy minimize()\n",
    "# Return optimal theta \n",
    "\n",
    "def optimalTheta(theta, X, y, reg = 0):\n",
    "    res = minimize(fun = loss, x0 = theta, args = (X, y, reg),  jac=True)\n",
    "    \n",
    "    return res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST: REMOVE IT! \n",
    "def test_loss(x, y, w, reg=0):\n",
    "    m = len(x)\n",
    "    total_error = 0.0 \n",
    "    for i in range(m):\n",
    "        total_error += (y[i] - (w*x[i]))**2\n",
    "    J_total = total_error / m\n",
    "    \n",
    "    return J_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST: REMOVE IT! \n",
    "def test_optimalTheta(theta, X, y, reg = 0):\n",
    "    res = minimize(fun = test_loss, x0 = theta, args = (X, y, reg),  jac = True)\n",
    "    \n",
    "    return res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7.37026612]\n"
     ]
    }
   ],
   "source": [
    "# TEST: REMOVE IT! \n",
    "initial_theta = 0\n",
    "theta_optim = test_optimalTheta(initial_theta, X_train[:,1], y_train)\n",
    "print(theta_optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. Run linear regression \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LinearRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.LinearRegression.html\">?<span>Documentation for LinearRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LinearRegression()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model (linear, generic)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#initial_theta = lin_reg.predict(X_train)[:,0]\n",
    "#initial_theta = np.zeros(X_train.shape[1])\n",
    "initial_theta = np.zeros(X_train.shape)[:,1]\n",
    "print(initial_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m initial_theta \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(X_train\u001b[38;5;241m.\u001b[39mshape)[:,\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# optimize \u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m theta_optim \u001b[38;5;241m=\u001b[39m \u001b[43moptimalTheta\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_theta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# print the result as a test \u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(theta_optim) \n",
      "Cell \u001b[1;32mIn[23], line 5\u001b[0m, in \u001b[0;36moptimalTheta\u001b[1;34m(theta, X, y, reg)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimalTheta\u001b[39m(theta, X, y, reg \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m----> 5\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39mx\n",
      "File \u001b[1;32mc:\\Users\\uzivatel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:691\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    689\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_cg(fun, x0, args, jac, callback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbfgs\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 691\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_bfgs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewton-cg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    693\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    694\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\uzivatel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:1362\u001b[0m, in \u001b[0;36m_minimize_bfgs\u001b[1;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, finite_diff_rel_step, xrtol, **unknown_options)\u001b[0m\n\u001b[0;32m   1359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maxiter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1360\u001b[0m     maxiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(x0) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[1;32m-> 1362\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_scalar_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1365\u001b[0m f \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun\n\u001b[0;32m   1366\u001b[0m myfprime \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mgrad\n",
      "File \u001b[1;32mc:\\Users\\uzivatel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:332\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[1;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[0;32m    328\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[1;32m--> 332\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43mScalarFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[1;32mc:\\Users\\uzivatel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:158\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[1;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl \u001b[38;5;241m=\u001b[39m update_fun\n\u001b[1;32m--> 158\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# Gradient evaluation\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(grad):\n",
      "File \u001b[1;32mc:\\Users\\uzivatel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[1;32m--> 251\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\uzivatel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\uzivatel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32mc:\\Users\\uzivatel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:76\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m     75\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[1;32mc:\\Users\\uzivatel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:71\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     70\u001b[0m fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfun(x, \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m \u001b[43mfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "# Find the optimal parameters \n",
    "# initialize theta \n",
    "initial_theta = np.zeros(X_train.shape)[:,1]\n",
    "# optimize \n",
    "theta_optim = optimalTheta(initial_theta, X_train[:,1], y_train)\n",
    "# print the result as a test \n",
    "print(theta_optim) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model data for plot e.g. (-50, 50, 100)\n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result (data points and the model)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the learning curve\n",
    "\n",
    "Let's compare our model’s training error with the validation error. These error values are the numbers output by our loss function relative to both datasets. One way to diagnose bias and variance errors is to plot the error relative to the training set and the validation data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to get training and testing error against the increasing number of training data \n",
    "# from 1 to range of y_train.size \n",
    "# store the training and testing error for later plot \n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function \n",
    "\n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the changing model trainin and testing error against the increasing number of training examples \n",
    "# (x: Number of training examples, y: Loss)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our error is high when the number of training examples increases, which is the exact opposite of what we should expect to see. This is definitely a high bias problem. Linear regression is just too simple to fit this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3. Run polynomial regression\n",
    "\n",
    "Addressing a simple linear model means adding more features which are no longer linear, but are instead the features being raised to higher powers. Explicitly, we will redefine $h$ as\n",
    "\n",
    "$$ h_\\theta(x) = \\theta^Tx = \\theta_0 + \\theta_1 * (waterLevel) + \\theta_2 * (waterLevel)^2 + ... +  \\theta_p * (waterLevel)^p $$\n",
    "$$ = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2^2 + ... +  \\theta_n x_n^p $$\n",
    "\n",
    "... we are  squaring, cubing, etc the columns of our $X$ matrix and then multiplying these new features by new parameters $\\theta$ and adding all that together. This is called **feature mapping** -- we are mapping the old features to a higher dimension. Let's code it up! We will also code up **feature normalization**, which is defined as\n",
    "\n",
    "$$ X_{norm} = \\frac{(X - \\mu)}{\\sigma} $$\n",
    "\n",
    "where $\\mu$ is the mean and $\\sigma$ is the standard deviation. We do feature normalization because after feature mapping, the values need to be rescaled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature transformation function to given degree of polynomial \n",
    "# and feature normalization \n",
    "\n",
    "def polyFeatures(X, p):\n",
    "    \"\"\"Function to transform input features X into \n",
    "       polynomial of defiend degree and  normalize the features! \n",
    "       Returns both, original X and the transformed normalizaed X_norm\n",
    "    \"\"\"\n",
    "    for i in np.arange(p):\n",
    "        dim = i + 2\n",
    "        X = np.insert(X, X.shape[1], np.power(X[:,1], dim), axis = 1)\n",
    "    \n",
    "    X_norm = X\n",
    "    # Column wise\n",
    "    means = np.mean(X_norm, axis=0)\n",
    "    X_norm[:, 1:] = X_norm[:, 1:] - means[1:]\n",
    "    stds = np.std(X_norm, axis = 0)\n",
    "    X_norm[:, 1:] = X_norm[:, 1:] / stds[1:]\n",
    "    \n",
    "    return X, X_norm    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to fit theta to our new higher dimensional $X$ matrix and plot the fit. \n",
    "\n",
    "### Fit polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design decision is to use polynomial model with degree 5 \n",
    "degree = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prepare polynomial X (transform) \n",
    "pass \n",
    "\n",
    "# Initialize theta \n",
    "pass \n",
    "\n",
    "# Optimize \n",
    "pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot data and the resulting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model data for plot (linear data X from -50 to 50)\n",
    "\n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot figures (data and resulting model)\n",
    "\n",
    "pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is definitely low training error, however, this fit is far too complex. The model is overfited (high variance), so we now have the opposite problem. Let's plot the new learning curve.\n",
    "\n",
    "#### Plot the learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare polynomial X_train and X_test\n",
    "\n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use (re-define) function to get training and testing error against the increasing number of training data \n",
    "# from 1 to range of y.size \n",
    "# update the initial theta \n",
    "\n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot result (x: Number of training examples, y: Loss)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training error is basically zero, but it would also be nice if the validation error was also low. The model does not generalize well to data that were not used to optimize its parameter values. \n",
    "\n",
    "### Task 4. Regularize the model \n",
    "\n",
    "and adjust regularizing parameter $\\lambda$\n",
    "\n",
    "Try using a $\\lambda$-value of $2$ and see what happens to our polynomial regression fit and the learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize theta \n",
    "pass\n",
    "\n",
    "# Optimize, reg = 2 \n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model data for plot \n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot (x: Change in water level, y: Water flowing out of the dam)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Udated learning curve function with reg parameter \n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run the function to get the data \n",
    "\n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot result (x: Number of training examples, y: Loss)\n",
    "\n",
    "pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the above plots show, we no longer fit the training set perfectly, however, our cross-validation error has significantly decreased to the point where it is almost the same as our training error. This is encouraging and an indicator of a model that can generalize well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5. Regularize the model with high lambda\n",
    "\n",
    "What happens if we set $\\lambda$ up to something  high, $100$ for example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the theta \n",
    "pass \n",
    "\n",
    "# optimize, reg = 100 \n",
    "\n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model data for plot \n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "\n",
    "pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prepare the data\n",
    "\n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "\n",
    "pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're back to the problem we had with our linear model; the model is underfit! When $\\lambda$ is too high, the values of $\\theta$ are pushed too close to zero and cannot fit the data well. As such, we do poorly on both the training and cross-validation data. Playing with the value of $\\lambda$ to find the best fit is the eternal struggle of machine learning, even in neural networks!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Discuss the effect of regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In written form, if you have finished in advance to the others. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
